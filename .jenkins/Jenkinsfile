// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
//
// Jenkins CI declaration.
//
// Scripting defs and the declarative pipeline is presented first.
// The stepsMap array describes the pipeline stages to what CI-agnostic scripts they map to.
//
// These CI-agnostic scripts are used as an intermediate dockerized layer above the ant build.xml
// The ant build.xml is never invoked directly.
//
//
// This Jenkinsfile is expected to work on any ci-cassandra.a.o clone.
// Functionality that depends upon ASF Infra and the canonical ci-cassandra.a.o setup (e.g. post-commit builds)
//  is required to quietly fail when run on other environments.
//
//
// Validate/lint this file using the following command
// `curl -X POST  -F "jenkinsfile=<.jenkins/Jenkinsfile" https://ci-cassandra.apache.org/pipeline-model-converter/validate`

def jdksSupported = ["11", "17"] 
def archsSupported = ["amd64", "arm64"]
def pythonsSupported = ["3.8", "3.11"] 
def jdkDefault = "11"
def pythonDefault = "3.8"
def stageResults = [:]

// Steps config
def buildSteps = [
  'jar': [script: 'build-jars.sh', toCopy: null],
  'artifacts': [script: 'build-artifacts.sh', toCopy: 'apache-cassandra-*.tar.gz,apache-cassandra-*.jar,apache-cassandra-*.pom'],
  'lint': [script: 'check-code.sh', toCopy: null],
  'debian': [script: 'build-debian.sh', toCopy: 'cassandra_*,cassandra-tools_*'],
  'redhat': [script: 'build-redhat.sh rpm', toCopy: '*.rpm'],
]
buildSteps.each() {
    it.value.put('type', 'build')
    it.value.put('splits', 1)
}

def testSteps = [
  'cqlsh-test': [splits: 1],
  'fqltool-test': [splits: 1],
  'test-cdc': [splits: 8],
  'test': [splits: 8],
  'test-trie': [splits: 8],
  'test-compression': [splits: 8],
  'stress-test': [splits: 1],
  'test-burn': [splits: 8],
  'long-test': [splits: 8],
  'test-oa': [splits: 8],
  'test-system-keyspace-directory': [splits: 8],
  'jvm-dtest': [splits: 8],
  'jvm-dtest-upgrade': [splits: 8],
  'simulator-dtest': [splits: 1],
  'dtest': [splits: 64],
  'dtest-novnode': [splits: 64],
  'dtest-offheap': [splits: 64],
  'dtest-large': [splits: 8],
  'dtest-large-novnode': [splits: 8],
  'dtest-upgrade': [splits: 64],
  'dtest-upgrade-novnode': [splits: 64],
  'dtest-upgrade-large': [splits: 64],
  'dtest-upgrade-novnode-large': [splits: 64],
]
testSteps.each() {
    it.value.put('type', 'test')
    it.value.put('script', '.build/docker/run-tests.sh')
    if (it.key.startsWith('dtest')) {
        it.value.put('python-dtest', true)
    }
}

def stepsMap = buildSteps + testSteps

// define matrix axes
def Map matrix_axes = [
    arch: archsSupported,
    jdk: jdksSupported,
    python: pythonsSupported,
    cython: ['yes', 'no'],
    step: stepsMap.keySet(),
    split: (1..64).toList() // needs to be max splits can be ??
]

def List _axes = getMatrixAxes(matrix_axes).findAll { axis ->
    (isArchEnabled(axis['arch'])) && // skip disabled archs
    (isJdkEnabled(axis['jdk'])) && // skip disabled jdks
    (isStageEnabled(axis['step'])) && // skip disabled steps
    !(axis['python'] != pythonDefault && 'cqlsh-test' != axis['step']) && // Use only python 3.8 for all tests but cqlsh-test
    !(axis['cython'] != 'no' && 'cqlsh-test' != axis['step']) && // cython only for cqlsh-test, disable for others
    !(axis['jdk'] != jdkDefault && 'cqlsh-test' == axis['step']) && // run cqlsh-test only with jdk11
    // Disable splits for all but proper stages
    !(axis['split'] > 1 && !stepsMap.findAll { entry -> entry.value.splits >= axis['split'] }.keySet().contains(axis['step'])) &&
    // run only the build types on non-amd64
    !(axis['arch'] != 'amd64' && stepsMap.findAll { entry -> 'build' == entry.value.type }.keySet().contains(axis['step']))
}

// Prepare tasks
def Map tasks = [
  jars: [failFast: !isPostCommit()],
  tests: [failFast: !isPostCommit()],
]
for (def axis in _axes) {
  def cell = axis
  def name = getStepName(cell, stepsMap[cell.step])
  tasks[cell.step == "jar" ? "jars" : "tests"][name] = { ->
    "${stepsMap[cell.step].type}"(stepsMap[cell.step], cell) // build() or test()
  }
}

pipeline {
  agent none
  options {
    skipDefaultCheckout()

    // FIXME problem with this retry approach is it retries the whole pipeline.  what we want is just cell retry
    //retry(2)
  }
  parameters {
    string(name: 'repository', defaultValue: scm.userRemoteConfigs[0].url, description: 'Cassandra Repository')
    string(name: 'branch', defaultValue: env.BRANCH_NAME, description: 'Branch')
    choice(name: 'architecture', choices: archsSupported + "all", description: 'Pick architecture. The ARM64 is disabled by default at the moment.')
    choice(name: 'jdk', choices: jdksSupported + "all", description: 'Pick JDK versions.')
    booleanParam(name: 'stage_artifacts', defaultValue: false, description: 'Disable to exlude stage') // return default to true
    booleanParam(name: 'stage_lint', defaultValue: false) // return default to true
    booleanParam(name: 'stage_debian', defaultValue: false) // return default to true
    booleanParam(name: 'stage_redhat', defaultValue: false) // return default to true
    booleanParam(name: 'stage_fqltool-test', defaultValue: false) // return default to true
    booleanParam(name: 'stage_cqlsh-test', defaultValue: false) // return default to true
    booleanParam(name: 'stage_test-cdc', defaultValue: false) // return default to true
    booleanParam(name: 'stage_test', defaultValue: false) // return default to true
    booleanParam(name: 'stage_test-trie', defaultValue: false) // return default to true
    booleanParam(name: 'stage_test-compression', defaultValue: false) // return default to true
    booleanParam(name: 'stage_stress-test', defaultValue: false) // return default to true
    booleanParam(name: 'stage_test-burn', defaultValue: false) // return default to true
    booleanParam(name: 'stage_long-test', defaultValue: false) // return default to true
    booleanParam(name: 'stage_test-oa', defaultValue: false) // return default to true
    booleanParam(name: 'stage_test-system-keyspace-directory', defaultValue: false) // return default to true
    booleanParam(name: 'stage_jvm-dtest', defaultValue: false) // return default to true
    booleanParam(name: 'stage_jvm-dtest-upgrade', defaultValue: false) // return default to true
    booleanParam(name: 'stage_simulator-dtest', defaultValue: false) // return default to true
    booleanParam(name: 'stage_dtest', defaultValue: false) // return default to true
    booleanParam(name: 'stage_dtest-novnode', defaultValue: false) // return default to true
    booleanParam(name: 'stage_dtest-offheap', defaultValue: false) // return default to true
    booleanParam(name: 'stage_dtest-large', defaultValue: false) // return default to true
    booleanParam(name: 'stage_dtest-large-novnode', defaultValue: false) // return default to true
    booleanParam(name: 'stage_dtest-upgrade', defaultValue: false) // return default to true
    booleanParam(name: 'stage_dtest-upgrade-novnode', defaultValue: false) // return default to true
    booleanParam(name: 'stage_dtest-upgrade-large', defaultValue: false) // return default to true
    booleanParam(name: 'stage_dtest-upgrade-novnode-large', defaultValue: false) // return default to true
  }
  environment {
    javaVersionsSupported = jdksSupported.join(',')
    javaVersionDefault = "${jdkDefault}"
  }
  stages {
    stage('jar') {
      steps {
        script {
          parallel(tasks['jars'])
        }
      }
    }
    stage('Tests') {
      when {
        expression { tasks['tests'].size() > 1 } // Skip if empty (failfast counts as an element)
      }
      steps {
        script {
          parallel(tasks['tests'])
        }
      }
    }
    stage('Summary') {
      steps {
        script {
          if(stageResults.find { "FAILURE" == it.value }) {
            currentBuild.result='FAILURE'
            error("Build failed due to failed stages")
          } else if(stageResults.find { "UNSTABLE" == it.value || "ABORTED" == it.value }) {
            if ("ABORTED" != currentBuild.result) { currentBuild.result='UNSTABLE' }
            echo("Build unstable due to unstable or aborted stages")
          }
        }
      }
    }
  }
  post { 
    always { 
      generateUnifiedTestReport()
      sendNotifications()
    }
  }
}

///////////////////////////
//// scripting support ////
///////////////////////////

def getStepName(cell, command) {
  arch = "amd64" == cell.arch ? "" : " ${cell.arch}"
  python = "cqlsh-test" != cell.step ? "" : " python${cell.python}"
  cython = "no" == cell.cython ? "" : " cython"
  split = command.splits > 1 ? " ${cell.split}/${command.splits}" : ""
  return "${cell.step}${arch} jdk${cell.jdk}${python}${cython}${split}"
}

/**
 * Return the default JDK defined by build.xml
 **/
def javaVersionDefault() {
  sh (returnStdout: true, script: 'grep \'property\\s*name=\"java.default\"\' build.xml | sed -ne \'s/.*value=\"\\([^\"]*\\)\".*/\\1/p\'').trim()
}

/**
 * Return the supported JDKs defined by build.xml
 **/
def javaVersionsSupported() {
  sh (returnStdout: true, script: 'grep \'property\\s*name=\"java.supported\"\' build.xml | sed -ne \'s/.*value=\"\\([^\"]*\\)\".*/\\1/p\'').trim()
}

/**
 * Is this a post-commit build (or a pre-commit build)
 **/
def isPostCommit() {
  // any build of a branch found on github.com/apache/cassandra is considered a post-commit (post-merge) CI run
  return params.repository && params.repository.contains("apache/cassandra") // no params exist first build
}

/**
 * Are we running on ci-cassandra.apache.org ?
 **/
def isCanonical() {
  return "${JENKINS_URL}".contains("ci-cassandra.apache.org")
}

def isStageEnabled(stage) {
  return params."stage_${stage}" || "jar" == stage
}

def isArchEnabled(arch) {
  return params.architecture == arch || "all" == params.architecture
}

def isJdkEnabled(jdk) {
  return params.jdk == jdk || "all" == params.jdk
}

/**
 * Renders build script into pipeline steps
 **/
def build(command, cell) {
  def build_script = ".build/docker/${command.script}"
  node(getNodeArch(cell.arch)) {
    withEnv(cell.collect { k, v -> "${k}=${v}" }) {
      ws("workspace/${JOB_NAME}/${BUILD_NUMBER}/${cell.step}/${cell.arch}/jdk-${cell.jdk}") {
          cleanWs(disableDeferredWipeout: true)
          fetchSource(cell.step, cell.arch, cell.jdk)
          def logfile = "build/${JOB_NAME}_${BUILD_NUMBER}_${cell.step}_jdk${cell.jdk}_${cell.arch}.log"

          catchError(buildResult: null, message: 'Build task failed', stageResult: 'FAILURE') {
              // pipe to tee needs pipefail
              sh label: 'RUNNING ${cell.step}...', script: "#!/bin/bash \n set -o pipefail ; ${build_script} ${cell.jdk} 2>&1 | tee ${logfile}"
              if ("jar" == step) { // only stash the project built files. all dependency libraries are restored from the local maven repo using `ant resolver-dist-lib`
                  stash name: "${cell.arch}_${cell.jdk}", useDefaultExcludes: false //, includes: '**/*.jar' //, includes: "*.jar,classes/**,test/classes/**,tools/**"
              }
              dir("build") {
                copyToNightlies("${command.toCopy}", "${STAGE_NAME}/jdk${cell.jdk}/${cell.arch}/")
              }
          }
          dir("build") {
            sh "xz -f *.log"
            archiveArtifacts artifacts: "**/*.log.xz", fingerprint: true
            copyToNightlies("**/*.log.xz", "${cell.step}/jdk${cell.jdk}/${cell.arch}/")
          }
          cleanAgent(build_script)
      }
    }
  }
}

def test(command, cell) {
  def splits = command.splits ? command.splits : 1 
  def python = cell.python
  def cython = cell.cython
  node(getNodeArch(cell.arch)) {
    withEnv(cell.collect { k, v -> "${k}=${v}" }) {
      ws("workspace/${JOB_NAME}/${BUILD_NUMBER}/${cell.step}/${cell.arch}/jdk-${cell.jdk}/python-${cell.python}") {
        cleanWs(disableDeferredWipeout: true)
        fetchSource(cell.step, cell.arch, cell.jdk)
        def logfile = "build/${JOB_NAME}_${BUILD_NUMBER}_${cell.step}_jdk${cell.jdk}_python_${cell.python}_${cell.cython}_${cell.arch}.log"

        // pipe to tee needs pipefail
        def script_vars = "#!/bin/bash \n set -o pipefail ; "
        script_vars = "${script_vars} python_version=\'${cell.python}\'"
        if ("cqlsh-test" == cell.step) {
            script_vars = "${script_vars} cython=\'${cell.cython}\'"
        }
        if (command.containsKey('python-dtest')) {
          // TODO – introduce parameters for cassandra-dtest repo
          checkout changelog: false, poll: false, scm: scmGit(branches: [[name: 'trunk']], extensions: [cloneOption(depth: 1, noTags: true, reference: '', shallow: true), [$class: 'RelativeTargetDirectory', relativeTargetDir: "${WORKSPACE}/build/cassandra-dtest"]], userRemoteConfigs: [[url: 'https://github.com/apache/cassandra-dtest']])
          script_vars = "${script_vars} cassandra_dtest_dir='${WORKSPACE}/build/cassandra-dtest'"
        }

        def stageError = true
        catchError(buildResult: null, message: 'Tests failed', stageResult: 'FAILURE') {
          sh label: "RUNNING TESTS ${cell.step}...", script: "${script_vars} .build/docker/run-tests.sh ${cell.step} '${cell.split}/${splits}' ${cell.jdk} 2>&1 | tee ${logfile}"
          dir("build") {
            junit testResults: "test/**/TEST*.xml,test/output/cqlshlib.xml,test/output/nosetests.xml", testDataPublishers: [[$class: 'StabilityTestDataPublisher']]
            sh "xz -f test/**/TEST*.xml || true ; xz -f test/output/cqlshlib.xml test/output/nosetests.xml || true"
          }
          stageError = false
        }
        if (stageError) {
          stageResults[getStepName(cell, command)] = "FAILURE"
        }
        dir("build") {
            sh "xz -f *.log"
            archiveArtifacts artifacts: "**/*.log.xz,test/logs/**,test/**/TEST*.xml.xz,test/output/cqlshlib.xml.xz,test/output/nosetests.xml.xz", fingerprint: true
            copyToNightlies("*.log.xz,test/logs/**", "${cell.step}/${cell.arch}/jdk${cell.jdk}/python${cell.python}/cython_${cell.cython}/" + "split_${cell.split}_${splits}".replace("/", "_"))
        }
        cleanAgent(".build/docker/run-tests.sh")
      }
    }
  }
}

def fetchSource(stage, arch, jdk) {
    if ("jar" == stage) {
        checkout changelog: false, scm: scmGit(branches: [[name: params.branch]], extensions: [cloneOption(depth: 1, noTags: true, reference: '', shallow: true)], userRemoteConfigs: [[url: params.repository]])
        sh "mkdir build"
    } else {
        unstash name: "${arch}_${jdk}"
    }
}

/**
 * Backward compatibility to support ci-cassandra.a.o Renders node architecture by real architecture name
 **/
def getNodeArch(arch) {
  switch(arch) {
    case "amd64":
      return "cassandra"
    case "arm64":
      return "cassandra-arm64"
    default:
      error("Unsupported architecture '${arch}'")
  }
}

def copyToNightlies(sourceFiles, remoteDirectory='') {
    if (!isCanonical() || "" == sourceFiles) {
        return;
    }

    def remotePath = remoteDirectory.startsWith("cassandra/") ? "${remoteDirectory}" : "cassandra/${JOB_NAME}/${BUILD_NUMBER}/${remoteDirectory}"
    def attempt = 1
    retry(9) {
        if (attempt > 1) { sleep(60 * attempt) }
        sshPublisher(
        continueOnError: true, failOnError: false,
        publishers: [
            sshPublisherDesc(
            configName: "Nightlies",
            transfers: [ sshTransfer( sourceFiles: sourceFiles, remoteDirectory: remotePath) ]
            )
        ])
    }
    echo "archived to https://nightlies.apache.org/${remotePath}"
}

def cleanAgent(job_name) {
  if (isCanonical()) {
    def maxJobHours = 12
    echo "Cleaning project, processes, docker for '${job_name}' on ${NODE_NAME}…" ;
    sh """
        git clean -qxdff -e build/test/jmh-result.json || true;
        if pgrep -xa docker || pgrep -af "build/docker" || pgrep -af "cassandra-builds/build-scripts" ; then docker system prune --all --force --filter "until=${maxJobHours}h" || true ; else  docker system prune --force --volumes || true ;  fi;
      """
  }
}

//  CASSANDRA-18130
def saveAgentReport() {
  if (isCanonical()) {
    //
    // echo "Updating disk usage report…";
    // sh """
    //     ( echo "----" ;
    //     echo \$(date) ;
    //     echo "${JOB_NAME} ${BUILD_NUMBER} ${STAGE_NAME}" ;
    //     du -xm / 2>/dev/null | sort -rn | head -n 30 ;
    //     df -h ) | tee -a \$(date +"%Y%m%d%H%M")-disk-usage-stats.txt
    //   """
    //   copyToNightlies("*-disk-usage-stats.txt", "cassandra/agents/${NODE_NAME}/disk-usage/")
    //   sh 'rm *-disk-usage-stats.txt'
  }
}

/////////////////////////////////////////
////// scripting support for summary ////
/////////////////////////////////////////

def generateUnifiedTestReport() {
  node(getNodeArch('amd64')) {
    checkout changelog: false, scm: scmGit(branches: [[name: params.branch]], extensions: [cloneOption(depth: 1, noTags: true, reference: '', shallow: true)], userRemoteConfigs: [[url: params.repository]])
    copyArtifacts filter: 'test/**/TEST*.xml.xz', fingerprintArtifacts: true, projectName: env.JOB_NAME, selector: specific(env.BUILD_NUMBER), target: "build/", optional: true
    if (fileExists('build/test/output')) {
      sh ".build/docker/_docker_run.sh bullseye-build.docker generate-test-report.sh"
      dir('build/') {
        sh "xz -f TESTS-TestSuites.xml"
        sh "tar -cf TESTS-html.tar test/html && xz -f TESTS-html.tar"
        archiveArtifacts artifacts: "TESTS-TestSuites.xml.xz,TESTS-html.tar.xz", fingerprint: true
        copyToNightlies('TESTS-TestSuites.xml.xz,TESTS-html.tar.xz')
      }
    }
  }
}

def sendNotifications() {
  if (isPostCommit() && isCanonical()) {
    // the following is expected only to work on ci-cassandra.apache.org
    try {
      script {
        changes = formatChangeLogChanges(currentBuild.changeSets)
        echo "changes: ${changes}"
      }
      slackSend channel: '#cassandra-builds', message: ":apache: <${BUILD_URL}|${currentBuild.fullDisplayName}> completed: ${currentBuild.result}. <https://github.com/apache/cassandra/commit/${GIT_COMMIT}|${GIT_COMMIT}>\n${changes}"
      emailext to: 'builds@cassandra.apache.org', subject: "Build complete: ${currentBuild.fullDisplayName} [${currentBuild.result}] ${GIT_COMMIT}", presendScript: 'msg.removeHeader("In-Reply-To"); msg.removeHeader("References")', body: emailContent()
    } catch (Exception ex) {
      echo 'failed to send notifications  ' + ex.toString()
    }
  }
}

def formatChangeLogChanges(changeLogSets) {
  def result = ''
  for (int i = 0; i < changeLogSets.size(); i++) {
    def entries = changeLogSets[i].items
    for (int j = 0; j < entries.length; j++) {
      def entry = entries[j]
      result = result + "${entry.commitId} by ${entry.author} on ${new Date(entry.timestamp)}: ${entry.msg}\n"
    }
  }
  return result
}

def emailContent() {
  return '''
  -------------------------------------------------------------------------------
  Build ${ENV,var="JOB_NAME"} #${BUILD_NUMBER} ${BUILD_STATUS}
  URL: ${BUILD_URL}
  -------------------------------------------------------------------------------
  Changes:
  ${CHANGES}
  -------------------------------------------------------------------------------
  Failed Tests:
  ${FAILED_TESTS,maxTests=500,showMessage=false,showStack=false}
  -------------------------------------------------------------------------------
  For complete test report and logs see https://nightlies.apache.org/cassandra/${JOB_NAME}/${BUILD_NUMBER}/
  '''
}

@NonCPS
def List getMatrixAxes(Map matrix_axes) {
    List axes = []
    matrix_axes.each { axis, values ->
        List axisList = []
        values.each { value ->
            axisList << [(axis): value]
        }
        axes << axisList
    }
    axes.combinations()*.sum()
}
